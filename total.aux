\relax 
\citation{lowe1999object}
\citation{bay2006surf}
\citation{dalal2005histograms}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{szegedy2014going}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{jia2014caffe}
\citation{srivastava2014dropout}
\citation{lin2013network}
\citation{szegedy2014going}
\citation{kawano14c}
\citation{bossard14}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Experimental Setup}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Models}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Food Datasets}{1}}
\citation{agrawal2014analyzing}
\citation{glorot2010understanding}
\citation{bossard14}
\citation{singh2012unsupervised}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Inception Cell. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ receptive field layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ receptive field after the MAX pooling layer. The output layer concatenate all its input layers.}}{2}}
\newlabel{incept}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experimental Discuss}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Pre-training and Fine-tuning}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Top-5 Accuracy in percent on fine-tuned, pre-trained and scratch model for two architectures}}{2}}
\newlabel{tab:ft}{{I}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Accuracy compared to other methods on Food-101 dataset}}{2}}
\citation{NIPS2014_Zhou}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of some responses of different GoogLeNet models in different layers for the same input image. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. }}{3}}
\newlabel{fig:sashimi}{{2}{3}}
\newlabel{relu}{{1}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Cosine similarity of the inceptions between fine-tuned models and scratch model for GoogLeNet}}{3}}
\newlabel{tab:cosg}{{III}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Training across the datasets}{3}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,research}
\bibcite{lowe1999object}{1}
\bibcite{bay2006surf}{2}
\bibcite{dalal2005histograms}{3}
\bibcite{krizhevsky2012imagenet}{4}
\bibcite{zeiler2014visualizing}{5}
\bibcite{simonyan2014very}{6}
\bibcite{szegedy2014going}{7}
\bibcite{Chatfield14}{8}
\bibcite{jia2014caffe}{9}
\bibcite{srivastava2014dropout}{10}
\bibcite{lin2013network}{11}
\bibcite{kawano14c}{12}
\bibcite{bossard14}{13}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Sparsity of the output for each unit in GoogLeNet inception for training data from Food101 in percent}}{4}}
\newlabel{tab:addlabel}{{V}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Cosine similarity of the layers between fine-tuned models and scratch model for AlexNet}}{4}}
\newlabel{tab:cosa}{{IV}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Top5 Accuracy for transferring from Food101 to subset of Food256}}{4}}
\newlabel{tab:cross}{{VI}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusion}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}}
\bibcite{agrawal2014analyzing}{14}
\bibcite{glorot2010understanding}{15}
\bibcite{singh2012unsupervised}{16}
\bibcite{NIPS2014_Zhou}{17}
