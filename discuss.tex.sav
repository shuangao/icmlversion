Training a CNN with millions of parameters on a small dataset could always lead to horrible overfitting. But the idea of supervised pre-training on some huge image datasets could preventing this problem in certain degree. Compare to other initialized strategies according to certain distributions, the pre-trained model is initialized according to the distribution of the specific task. Indeed, this initialization has certain bias as there is no single dataset including all the invariance for natural images\cite{agrawal2014analyzing}, but this bias could be reduced as the pre-trained image dataset increases and the fine-tuning can be benefit from this initialization.
\subsection{Pre-training and Fine-tuning}
We conduct several experiments on both architectures and use different training initialization strategies for both Food-256 and Food-101 datasets. The scratch model is initialized with Gaussian distribution for AlexNet and Xavier algorithm\cite{glorot2010understanding}, which automatically determines the scale of initialization based on the number of input and output neurons. These two initializations are used for training the model for the original ImageNet task. The pre-trained models and fine-tune models are initialized with the weights trained from ImageNet. For the pre-trained models, we just re-train the output layers while all the layers are re-trained for the fine-tune models.
\begin{table}[htbp]
  \centering
  \caption{Top-5 Accuracy in percent on fine-tuned, pre-trained and scratch model for two architectures}
    \begin{tabular}{r|cc|cc}
    \toprule
          & \multicolumn{2}{c|}{AlexNet} & \multicolumn{2}{c}{GoogLeNet} \\    \midrule
     & Food-101   & Food-256   & Food-101   & Food-256 \\
    Fine-tune & \textbf{88.12} & \textbf{85.59} & \textbf{93.51} & \textbf{90.66} \\
    Pre-trained &76.49	&79.26&	82.84	&83.77\\
    Scratch & 78.18 & 75.35 & 90.45 & 81.20 \\
    \bottomrule
    \end{tabular}%
  \label{tab:ft}%
\end{table}%



\begin{figure*}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.5]{fig/sashimi.png}\\
  \caption{Visualization of some responses of different GoogLeNet models in different layers for the same input image. Conv1 is the 1st convolutional layer and Inception\_5b is the last convolutional layer. }
   \label{fig:sashimi}
\end{figure*}
From Table \ref{tab:ft} we can see that fine-tune from pre-trained model can boost the performance of the CNN for a specific task. It is interesting to see that for the Food-101 task, the accuracy of  the scratch models outperforms the pre-trained models.

In Figure \ref{fig:sashimi} we visualized the responses of the pre-trained GoogLeNet model and fined-tuned GoogLeNet model for the same input image for some layers. We can see that the responses of the lower layer are similar as the lower level features are similar. Then we can see that the decisions made by these two models is totally different. Since only the last layer (auxiliary classifier) of the pre-trained model is optimized, we can infer that the higher level features are more important which is consistent with our intuition. From Table \ref{tab:ft} we can see that GoogLeNet always performances better than AlexNet on both datasets. This implies that the higher level features of GoogLeNet are more reasonable compared to AlexNet and this is due to the special architecture of its basic unit, Inception. 
 
Table \ref{tab:cosa} and \ref{tab:cosg} show the weights' cosine similarity of each layer between the fine-tuned models and their pre-trained models. From the results we can see that the weights in the low layer are more similar which implies that these two architectures can learn the hierarchical features as the low level features are similar for most of the tasks and the difference of the objects is determined by the combination of these low level features. From Table \ref{tab:cosa}, we can see that, in AlexNet the weights of the pre-trained and fine-tuned models are extremely similar. This can be caused by two reasons:
\begin{itemize}
  \item Small receptive filed. Since ReLUs are used in Both architectures, vanishing gradients do not exist. Rectified activation function is mathematically given by
      \begin{equation}\label{relu}
        h = \max ({w^T}x,0) = \left\{ {\begin{array}{*{20}{c}}
{{w^T}x}&{{w^T}x > 0}\\
0&{else}
\end{array}} \right.
      \end{equation}

    The ReLU is inactivated when its input is below 0 and its partial derivative is 0. Sparseness can boost the performance of the linear classifier on top, but on the other hand, sparseness will make the more difficult to train especially for fine-tuning. The derivative of the kernel is $\frac{{\partial J}}{{\partial w}} = \frac{{\partial J}}{{\partial y}}\frac{{\partial y}}{{\partial w}} = \frac{{\partial J}}{{\partial y}}*x$ where $\frac{{\partial J}}{{\partial y}}$ denotes the partial derivative of the activation function, $y=w^Tx$ and $x$ denotes the inputs of the layer. The sparseness of the input could lead to sparse kernel derivative. Therefore, the filters of the fine-tuned AlexNet is extremely similar. Compared to large receptive field used in AlexNet, GoogLeNet employs 2 additional $n\times n\_reduced$ convolutional layers before the $3\times 3$ and $5\times 5$ convolutional layers. Even though the most critical purpose of these two $1\times 1$ convolutional layer is for computational efficiency, these 2 convolutional layers tend to squeeze the sparse input and generate a dense output as the input of the next layer.
  \item The pooling strategy. In AlexNet, max pooling is applied to all the pooling layers between several convolution layers and in back propagation, the max pooling layer will pass the error to the place where it came from. Since it only came from one place of the receptive field, the back propagation error is sparse and it keeps the kernels of the convolution layers stable. In GoogLeNet, even though, there is a max pooling layer in every inception, there are other 3 back propagation errors, from $5\times 5\_reduce$ and $3\times 3\_reduce$ that can affect the weights of the previous inception.
\end{itemize}
\begin{table}[htbp]
  \centering
  \caption{Cosine similarity of the inceptions between fine-tuned models and scratch model for GoogLeNet}
    \begin{tabular}{r|cccccc}
    \toprule
    \multicolumn{7}{c}{food256} \\
    \midrule
          & \multicolumn{1}{l}{1x1} & \multicolumn{1}{l}{3x3\_reduce} & \multicolumn{1}{l}{3x3} & \multicolumn{1}{l}{5x5\_reduce} & \multicolumn{1}{l}{5x5} & \multicolumn{1}{l}{pool\_proj } \\
    inception\_3a & 0.72  & 0.72  & 0.64  & 0.67  & 0.73  & 0.69 \\
    inception\_3b & 0.59  & 0.64  & 0.53  & 0.70  & 0.60  & 0.56 \\
    inception\_4a & 0.46  & 0.53  & 0.54  & 0.50  & 0.67  & 0.38 \\
    inception\_4b & 0.55  & 0.58  & 0.63  & 0.52  & 0.69  & 0.41 \\
    inception\_4c & 0.63  & 0.64  & 0.63  & 0.57  & 0.68  & 0.52 \\
    inception\_4d & 0.60  & 0.62  & 0.60  & 0.58  & 0.68  & 0.50 \\
    inception\_4e & 0.60  & 0.61  & 0.67  & 0.61  & 0.68  & 0.50 \\
    inception\_5a & 0.51  & 0.53  & 0.58  & 0.48  & 0.60  & 0.39 \\
    inception\_5b & 0.40  & 0.44  & 0.50  & 0.41  & 0.59  & 0.40 \\  \toprule
    \multicolumn{7}{c}{food101} \\ \midrule
          & \multicolumn{1}{l}{1x1 } & \multicolumn{1}{l}{3x3\_reduce} & \multicolumn{1}{l}{3x3} & \multicolumn{1}{l}{5x5\_reduce} & \multicolumn{1}{l}{5x5} & \multicolumn{1}{l}{pool\_proj } \\
    inception\_3a & 0.71  & 0.72  & 0.63  & 0.67  & 0.73  & 0.68 \\
    inception\_3b & 0.56  & 0.63  & 0.50  & 0.71  & 0.60  & 0.53 \\
    inception\_4a & 0.43  & 0.50  & 0.50  & 0.47  & 0.62  & 0.36 \\
    inception\_4b & 0.48  & 0.52  & 0.57  & 0.50  & 0.67  & 0.35 \\
    inception\_4c & 0.57  & 0.61  & 0.59  & 0.53  & 0.63  & 0.47 \\
    inception\_4d & 0.54  & 0.58  & 0.53  & 0.54  & 0.64  & 0.44 \\
    inception\_4e & 0.53  & 0.54  & 0.61  & 0.55  & 0.62  & 0.42 \\
    inception\_5a & 0.43  & 0.47  & 0.53  & 0.45  & 0.57  & 0.34 \\
    inception\_5b & 0.36  & 0.39  & 0.46  & 0.38  & 0.52  & 0.37 \\
    \bottomrule
    \end{tabular}%
  \label{tab:cosg}%
\end{table}%


\begin{table}[htbp]
  \centering
  \caption{Cosine similarity of the layers between fine-tuned models and scratch model for AlexNet}
    \begin{tabular}{r|ccccccc}
    \toprule
          & conv1 & conv2 & conv3 & conv4 & conv5 & fc6   & fc7 \\
    \midrule
    food256 & 0.997 & 0.987 & 0.976 & 0.976 & 0.978 & 0.936 & 0.923 \\
    food101 & 0.996 & 0.984 & 0.963 & 0.960 & 0.963 & 0.925 & 0.933 \\
    \bottomrule
    \end{tabular}%
  \label{tab:cosa}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'google'
\begin{table*}[htbp]
  \centering
  \caption{Sparsity of the output for each unit in GoogLeNet inception for training data from Food101 in percent}
    \begin{tabular}{r|cccccc}
    \toprule
          & 1x1  & 3x3\_reduce & 3x3  & 5x5\_reduce & 5x5  & pool\_proj  \\
    \midrule
    inception\_3a & $69.3\pm 1.3$  & $69.6 \pm 1.1$  & $80.0\pm  1.0$& $64.1\pm  2.2$& $75.8\pm  1.6$& $76.2\pm 5.4$\\
    inception\_3b & $92.8 \pm 0.9$&$ 76.5 \pm 0.9$& $94.7\pm 0.9 $&$ 71.6 \pm 2.3 $&$ 94.4\pm 0.5 $&$ 94.7 \pm 1.6$\\
    inception\_4a & $90.9 \pm 0.9$& $70.0\pm 1.2 $& $93.8\pm 1.1 $& $63.3\pm 4.0 $& $91.9\pm 1.8 $& $95.1\pm 2.0$\\
    inception\_4b & $71.9 \pm 1.6$& $67.5\pm 1.2$ & $75.4\pm  1.0$& $58.5 \pm 2.6$& $78.9\pm  1.6$& $85.6\pm 3.6$\\
    inception\_4c & $75.1 \pm 2.4$& $72.6 \pm 1.3$& $81.0\pm 2.0$ & $66.3\pm 6.1 $& $79.7 \pm 3.6$& $88.1\pm 3.3$\\
    inception\_4d & $87.3 \pm 2.7$& $78.0 \pm 2.2$& $88.0\pm 1.6$& $67.9\pm 3.1 $& $88.9\pm 2.8 $& $93.0\pm 2.2$\\
    inception\_4e & $91.8\pm  1.1$& $62.3\pm 2.2 $& $91.0\pm 2.5 $& $49.5 \pm 3.7$& $94.0 \pm 1.0$& $92.3\pm 1.5$\\
    inception\_5a & $78.7 \pm 1.6$& $66.5\pm  1.7$& $82.3\pm 2.6 $& $59.9\pm 3.2 $& $86.4\pm 2.3 $& $87.1\pm 2.6$\\
    inception\_5b & $88.2\pm 2.3 $& $86.8 \pm 1.6$&$ 83.3\pm 4.4$ & $84.0\pm 3.1 $& $81.4\pm 5.3$  & $94.7\pm 1.5$\\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table*}%



\subsection{Training across the datasets}
From the previous experiments we can see that pre-training on the ImageNet dataset can boost the performance of the deep convolutional neural network and the knowledge learned from the general recognition problem can be successfully transferred into our specific area. In this part, we will discuss the generalization ability within the our food recognition area.  Zhou et al. trained AlexNet for Scene Recognition across two datasets with identical categories\cite{NIPS2014_Zhou}. But for more complex situation, such as two similar datasets with different categories, we are very interested in exploring whether Deep CNN can still successfully handle. Therefore, we conduct the following experiment to stimulate a more complex real world problem: transferring the knowledge from the fine-tuned Food-101 model on Food-256 dataset and continue fine-tune it on Food-256. To make the experiment more practical, we limited the number of samples per category from Food-256 for training, because in practise, if we want to build a our own model from Deep CNN, the resource is limited and it is exhausted to collect hundreds of labeled images for each category.

The Food-101 and Food-256 datasets share about 46 categories of food even though the images in the same category may vary across these two datasets. The types of food in Food-101 are mainly western style while most types of food in Food-256 are typical Asian foods. We compared the top-5 accuracy of different size of subset of Food-256 on different pre-trained model and get Table \ref{tab:cross}. The ImageNet columns denote the pre-trained model trained only on ImageNet images and the Food101\_ft columns denote the pre-trained model trained on ImageNet images and then fine-tuned on Food-101. From the result of Table \ref{tab:cross} we can see that, 

\begin{table}[htbp]
  \centering
  \caption{Top5 Accuracy for transferring from Food101 to subset of Food256}
    \begin{tabular}{c|cc|cc}
    \toprule
          & \multicolumn{2}{c|}{AlexNet} & \multicolumn{2}{c}{GoogLeNet} \\
    \midrule
    instances per class & ImageNet  & Food101\_ft    &  ImageNet  & Food101\_ft \\ \midrule
    20    & 68.80  & \textbf{75.12} & 74.54 & \textbf{77.77} \\
    30    & 73.15 & \textbf{77.02} & 79.21 & \textbf{81.06} \\
    40    & 76.04 & \textbf{80.23} & 81.76 & \textbf{83.52} \\
    50    & 78.90  & \textbf{81.66} & 84.22 & \textbf{85.84} \\
    all    & 85.59 &                       & 90.66 &                         \\
    \bottomrule
    \end{tabular}%
  \label{tab:cross}%
\end{table}%

