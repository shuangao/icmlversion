Over the recent few years, Convolutional Neural Network (CNN) shows its potential to replace the human engineered features, such as SIFT\cite{lowe1999object}, SURF\cite{bay2006surf} and HOG\cite{dalal2005histograms} etc, in real object recognition tasks.  The success of CNN on large scale image set started from Krizhevsky et al\cite{krizhevsky2012imagenet} and their 8 layer model AlexNet in 2012 ImageNet Large-Scale Visual Recognition Challenge (ILSVRC2012), reaching a on top-5 83\% accuracy . Soon after, many attempts have been made to improve the model of Krizhevsky.
By reducing the size of the receptive field and stride, Zeiler and Fergus improve AlexNet by 1.7\% on top 5 accuracy\cite{zeiler2014visualizing}. With the help of high performances computing systems, such as GPU and large scale distributed clusters, it is possible for researchers to explore larger and more complex architecture. By both adding extra convolutional layers between two pooling layers and reduced the receptive window size, Simonyan and Zisserman built a 19 layer very deep CNN and achieved 92.5\% top-5 accuracy\cite{simonyan2014very}. While the AlexNet-like deep CNNs conquered ILSVRC, Szegedy et al built a 22 layers deep network, GoogLeNet \cite{szegedy2014going} and won the 1st prize on ILSVRC2014, reaching a astonishing 93.33\% top-5 accuracy.

Since these CNN models are trained on very large image data set, they have strong generalization ability and can be applied in many other scenarios. Applying the model pre-trained from ImageNet dataset on other object recognition dataset shows some impressive results.
Zeiler et al. applied their pre-trained model on Caltech-256 with just 15 instances per class to fine-tune the model and improved the previous state of the art in which about 60 instances are used for training, by almost 10\%\cite{zeiler2014visualizing}.
Chatfield et al used their pre-trained model on VOC2007 dataset and outperformed the previous state of the art by 0.9\%\cite{Chatfield14}.

Unlike the local features such as SIFT or SURF, which presents an intuitive interpretation of spatial property that is invariant with some transformations such as scaling and rotation, we still don't have enough knowledge to understand the visual features of CNN learned in each layer. Training a large deep CNN on real recognition problem is always a complicated task. The model contains hundreds of millions of parameters to learn and lots of hyper-parameters that can affect its performance. However, the truth that deep CNN outperforms other shallow models by a large margin in some real image recognition tasks encourages researchers to build deep architecture with powerful high performance hardware and larger datasets. With the help of high performance GPU clusters and data argumentation, people are more enthusiastic to explore bigger network on complex recognition problems without much interpretation which makes other researchers difficult to follow. In this paper, we try to apply the two kinds of deep CNN model, AlexNet and GoogLeNet, on a specific real learning problem, food recognition, and try to reveal some tricks in fine-tuning the existing CNN architecture on this problem.

The rest of this paper is organized as follow: in Section \ref{exp}, the two food image datasets and two deep CNN architectures are introduced. In Section \ref{exp}, some experimental results are shown and we also compare the performance between the deep CNNs as well as some traditional methods on these two datasets. Some discussion of the Inception's architecture and statistics of deep CNNs are shown. We also show some fine-tuning results when the training examples are rare for each class.
