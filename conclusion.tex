In this paper, we compare two different deep convolutional neural network architectures and their transferring ability on food datasets. Both architectures show their potential on generalization ability and we provide the state-of-the-art on both Food-101 dataset and Food-256 dataset using fine-tuned GoogLeNet. GoogLeNet shows its strong ability on transferring the knowledge between different tasks with the help of the specially designed unit, Inception.
%Intensively used $1\times 1$ convolutional layer in Inception reduces both computational cost and training complexity which leads to the final success of the whole architecture.
We find that not only does the intensively used $1\times 1$ convolutional layer in Inception reduce the computational cost, but it also helps to train the filters in the following layer. The filters after the $1\times 1$ convolutional layer can be trained more efficiently while the $1\times 1$ convolutional layer provides squeezed input and this helps GoogLeNet learn more complex high-level features.
Moreover, in a more practical situation such as transfer learning within a specific area with a few labeled instances for the target set, both of these two architectures show some encouraging results in transferring knowledge across the dataset, reaching around 95\% of the accuracy trained on full dataset with just half data.

