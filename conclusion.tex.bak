In this paper, we compared two different deep convolutional neural network architectures and their transferring ability on food datasets. Both architectures have shown their potential on generalization ability and we provide the state-of-the-art using fine-tuned GoogLeNet on Food-101 dataset, the winner of ILSVRC2014, shows its great ability on transferring the knowledge between different tasks with the help of the special designed unit, Inception. Intensively used $1\times 1$ convolutional layers in Inception reduces both computational cost and training complexity which leads to the final success of the whole architecture. Moreover, in a more practical situation such as learning with a few labeled instances, both of these two architectures show 